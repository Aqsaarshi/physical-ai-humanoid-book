"use strict";(globalThis.webpackChunkphysical_ai_humanoid_book=globalThis.webpackChunkphysical_ai_humanoid_book||[]).push([[316],{8453(e,n,i){i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},9439(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"chapter3","title":"Chapter 3: Sensing and Perception","description":"3.1 Introduction to Sensors","source":"@site/docs/chapter3.md","sourceDirName":".","slug":"/chapter3","permalink":"/physical-ai-humanoid-book/docs/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/Aqsaarshi/physical-ai-humanoid-book/tree/main/docs/chapter3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Robotics Fundamentals","permalink":"/physical-ai-humanoid-book/docs/chapter2"},"next":{"title":"Chapter 4: Actuation and Control","permalink":"/physical-ai-humanoid-book/docs/chapter4"}}');var o=i(4848),r=i(8453);const t={},a="Chapter 3: Sensing and Perception",c={},l=[{value:"3.1 Introduction to Sensors",id:"31-introduction-to-sensors",level:2},{value:"Types of Sensors",id:"types-of-sensors",level:3},{value:"Sensor Characteristics",id:"sensor-characteristics",level:3},{value:"3.2 Vision Systems",id:"32-vision-systems",level:2},{value:"Cameras (Monocular, Stereo, Depth)",id:"cameras-monocular-stereo-depth",level:3},{value:"Image Processing Fundamentals",id:"image-processing-fundamentals",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:3},{value:"3.3 Other Perception Modalities",id:"33-other-perception-modalities",level:2},{value:"Lidar and Radar",id:"lidar-and-radar",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"Example",id:"example",level:2},{value:"Basic Image Processing for Robot Navigation",id:"basic-image-processing-for-robot-navigation",level:3},{value:"Diagram",id:"diagram",level:2},{value:"Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline",id:"figure-31-robot-sensor-suite-and-vision-processing-pipeline",level:3},{value:"Mini-Task",id:"mini-task",level:2},{value:"Sensor Selection for an Autonomous Mobile Robot",id:"sensor-selection-for-an-autonomous-mobile-robot",level:3},{value:"Quiz",id:"quiz",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-sensing-and-perception",children:"Chapter 3: Sensing and Perception"})}),"\n",(0,o.jsx)(n.h2,{id:"31-introduction-to-sensors",children:"3.1 Introduction to Sensors"}),"\n",(0,o.jsx)(n.p,{children:"Sensors are critical components in any physical AI system, acting as the primary interface between the robot and its environment. They convert physical quantities (light, sound, pressure, distance, etc.) into electrical signals that the robot's control system and AI algorithms can interpret."}),"\n",(0,o.jsx)(n.h3,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,o.jsx)(n.p,{children:"Robots utilize a wide array of sensors, each designed for specific perception tasks:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision Sensors"}),": Cameras (monocular, stereo, RGB-D) provide visual information for object recognition, navigation, and scene understanding."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proximity and Range Sensors"}),": Ultrasonic sensors, infrared sensors, lidar, and radar are used to detect the presence of objects and measure distances."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tactile and Force Sensors"}),": Provide information about contact, pressure, and forces exerted during manipulation, enabling compliant interaction."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proprioceptive Sensors"}),": Internal sensors like encoders, accelerometers, and gyroscopes measure the robot's own state, such as joint angles, velocity, and orientation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Sensors"}),": Microphones for sound detection, speech recognition, and sound source localization."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-characteristics",children:"Sensor Characteristics"}),"\n",(0,o.jsx)(n.p,{children:"Understanding sensor characteristics is vital for selecting the right sensor for a given task:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": How close a sensor's measurement is to the true value."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Precision (Repeatability)"}),": How close repeated measurements are to each other under the same conditions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resolution"}),": The smallest change in the physical quantity that the sensor can detect."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Range"}),": The minimum and maximum values that the sensor can measure."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Response Time"}),": How quickly the sensor reacts to changes in the measured quantity."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"32-vision-systems",children:"3.2 Vision Systems"}),"\n",(0,o.jsx)(n.p,{children:"Vision systems are one of the most powerful perception modalities for robots, enabling them to 'see' and interpret their surroundings."}),"\n",(0,o.jsx)(n.h3,{id:"cameras-monocular-stereo-depth",children:"Cameras (Monocular, Stereo, Depth)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monocular Cameras"}),": Standard 2D cameras, providing rich texture and color information but lacking direct depth perception."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Cameras"}),": Mimic human binocular vision using two cameras separated by a baseline to estimate depth through triangulation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Depth Cameras (RGB-D)"}),": Combine an RGB camera with a depth sensor (e.g., structured light, time-of-flight) to provide color images alongside per-pixel depth information."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"image-processing-fundamentals",children:"Image Processing Fundamentals"}),"\n",(0,o.jsx)(n.p,{children:"Raw image data from cameras needs processing to extract meaningful information:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Filtering"}),": Noise reduction, edge detection, and smoothing."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Segmentation"}),": Dividing an image into regions or objects."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature Extraction"}),": Identifying key points, lines, or contours that represent salient aspects of the image."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Advanced AI algorithms, particularly deep learning models (e.g., Convolutional Neural Networks), are used for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection"}),": Identifying the presence and location of objects within an image (e.g., bounding boxes)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Recognition"}),": Classifying detected objects into predefined categories."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pose Estimation"}),": Determining the 3D position and orientation of objects."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"33-other-perception-modalities",children:"3.3 Other Perception Modalities"}),"\n",(0,o.jsx)(n.p,{children:"Beyond vision, other sensors provide crucial information, especially in conditions where vision is limited or insufficient."}),"\n",(0,o.jsx)(n.h3,{id:"lidar-and-radar",children:"Lidar and Radar"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lidar (Light Detection and Ranging)"}),": Uses pulsed laser light to measure distances to objects, creating detailed 3D maps of the environment. Excellent for autonomous navigation and mapping."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Radar (Radio Detection and Ranging)"}),": Uses radio waves to detect objects and measure their velocity and distance. More robust in adverse weather conditions (fog, rain) than lidar or cameras."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,o.jsx)(n.p,{children:"These sensors measure the forces and torques applied at a robot's gripper or joints. They are essential for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Compliant Motion"}),": Enabling robots to perform delicate tasks without damaging objects or exerting excessive force."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Ensuring safety and natural interaction by detecting contact forces."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": Allowing robots to handle objects with varying stiffness and weight."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,o.jsx)(n.p,{children:"Proprioceptive sensors provide feedback about the robot's internal state, crucial for precise control and motion:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoders"}),": Measure the rotational or linear position of joints."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accelerometers"}),": Measure linear acceleration, useful for detecting movement and orientation changes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gyroscopes"}),": Measure angular velocity, used for determining orientation and maintaining balance."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"IMUs (Inertial Measurement Units)"}),": Combine accelerometers and gyroscopes to provide comprehensive motion and orientation data."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,o.jsx)(n.h3,{id:"basic-image-processing-for-robot-navigation",children:"Basic Image Processing for Robot Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Concept"}),": A common task for mobile robots is to navigate an environment using visual information. This example outlines a simplified image processing pipeline to detect a specific color (e.g., a green line or object) for navigation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Image Acquisition"}),": A robot's monocular camera captures an image of the environment."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Input"}),": ",(0,o.jsx)(n.code,{children:"color_image"})," (e.g., a NumPy array representing RGB pixel values)."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Color Space Conversion"}),": Convert the RGB image to a more suitable color space for color detection, such as HSV (Hue, Saturation, Value)."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Reason"}),": HSV separates color information (Hue) from intensity (Value), making color thresholding more robust to lighting changes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Operation"}),": ",(0,o.jsx)(n.code,{children:"hsv_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)"})," (using OpenCV)."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Color Thresholding"}),": Define a range for the target color in HSV space and create a binary mask. Pixels within the range become white (255), and others become black (0)."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Example (Green Color)"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"lower_green = np.array([30, 40, 40])"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"upper_green = np.array([80, 255, 255])"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"mask = cv2.inRange(hsv_image, lower_green, upper_green)"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Morphological Operations (Optional)"}),": Clean up the mask by removing small noise (erosion) or filling small holes (dilation)."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Operation"}),": ",(0,o.jsx)(n.code,{children:"mask = cv2.erode(mask, kernel, iterations=1)"})," then ",(0,o.jsx)(n.code,{children:"mask = cv2.dilate(mask, kernel, iterations=1)"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Contour Detection"}),": Find contours (outlines of objects) in the binary mask."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Operation"}),": ",(0,o.jsx)(n.code,{children:"contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Object Localization"}),": If a significant contour (e.g., the largest green object) is found, calculate its centroid or bounding box."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.em,{children:"Decision"}),": Use the centroid's horizontal position to determine if the robot needs to turn left, right, or go straight to center the object."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Robot Action"}),": Based on the localization of the green object, the robot's control system would adjust motor speeds to steer towards or along the green path."]}),"\n",(0,o.jsx)(n.p,{children:"This simplified example demonstrates how raw sensor data (an image) is processed through various stages to extract actionable information for robot navigation."}),"\n",(0,o.jsx)(n.h2,{id:"diagram",children:"Diagram"}),"\n",(0,o.jsx)(n.h3,{id:"figure-31-robot-sensor-suite-and-vision-processing-pipeline",children:"Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Description"}),": A block diagram showing two main parts:"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Part 1: Robot Sensor Suite"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Illustrate a robot body with various sensors placed around it."}),"\n",(0,o.jsxs)(n.li,{children:["Labels should include: ",(0,o.jsx)(n.strong,{children:"Camera"}),", ",(0,o.jsx)(n.strong,{children:"Lidar Sensor"}),", ",(0,o.jsx)(n.strong,{children:"Ultrasonic/IR Sensors"}),", ",(0,o.jsx)(n.strong,{children:"Force/Tactile Sensor (on gripper)"}),", ",(0,o.jsx)(n.strong,{children:"IMU (internal)"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"Arrows should show data flowing from each sensor towards a central processing unit."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Part 2: Vision Processing Pipeline"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["A sequential flow starting with '",(0,o.jsx)(n.strong,{children:"Image Acquisition (Camera)"}),"'."]}),"\n",(0,o.jsxs)(n.li,{children:["Flows to '",(0,o.jsx)(n.strong,{children:"Color Space Conversion (e.g., RGB to HSV)"}),"'."]}),"\n",(0,o.jsxs)(n.li,{children:["Then to '",(0,o.jsx)(n.strong,{children:"Color Thresholding / Segmentation"}),"'."]}),"\n",(0,o.jsxs)(n.li,{children:["Followed by '",(0,o.jsx)(n.strong,{children:"Morphological Operations (Erode/Dilate)"}),"'."]}),"\n",(0,o.jsxs)(n.li,{children:["Leading to '",(0,o.jsx)(n.strong,{children:"Feature Extraction / Object Detection"}),"'."]}),"\n",(0,o.jsxs)(n.li,{children:["Finally, results in '",(0,o.jsx)(n.strong,{children:"Actionable Information for Control (e.g., Turn Left/Right)"}),"'."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Connect the output of the Sensor Suite to the input of the Vision Processing Pipeline, emphasizing how raw sensor data is transformed into meaningful information for robot decision-making."}),"\n",(0,o.jsx)(n.h2,{id:"mini-task",children:"Mini-Task"}),"\n",(0,o.jsx)(n.h3,{id:"sensor-selection-for-an-autonomous-mobile-robot",children:"Sensor Selection for an Autonomous Mobile Robot"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Objective"}),": Propose a sensor suite for an autonomous mobile robot (AMR) designed to navigate an indoor warehouse environment, avoiding obstacles and locating specific inventory items."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Identify key perception needs"}),": What kind of information does the AMR need to gather from its environment (e.g., precise localization, obstacle detection, object identification)?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Select appropriate sensors"}),": Based on the perception needs, choose a combination of sensors discussed in this chapter (e.g., Lidar, cameras, ultrasonic)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Justify your choices"}),": Explain why each selected sensor is suitable for the AMR's task, considering factors like environment, accuracy, and cost."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Example"}),": For obstacle avoidance, a Lidar sensor provides precise 360-degree depth information, while ultrasonic sensors can detect nearby objects economically. A monocular camera with object detection AI can identify inventory items."]}),"\n",(0,o.jsx)(n.h2,{id:"quiz",children:"Quiz"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Which sensor type is primarily used to provide visual information for object recognition and navigation?\na)  Ultrasonic sensors\nb)  Force/Torque sensors\nc)  Cameras\nd)  Encoders\n",(0,o.jsx)(n.em,{children:"Answer: c)"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["What does the term 'resolution' refer to in the context of sensor characteristics?\na)  How close a sensor's measurement is to the true value.\nb)  How quickly the sensor reacts to changes.\nc)  The smallest change in the physical quantity that the sensor can detect.\nd)  The maximum range the sensor can measure.\n",(0,o.jsx)(n.em,{children:"Answer: c)"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Stereo cameras primarily estimate depth through which principle?\na)  Time-of-flight\nb)  Structured light\nc)  Triangulation\nd)  Monocular vision\n",(0,o.jsx)(n.em,{children:"Answer: c)"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Which image processing operation is used to divide an image into regions or objects?\na)  Filtering\nb)  Segmentation\nc)  Feature Extraction\nd)  Thresholding\n",(0,o.jsx)(n.em,{children:"Answer: b)"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Which sensor technology uses pulsed laser light to create detailed 3D maps of the environment and is excellent for autonomous navigation?\na)  Radar\nb)  Lidar\nc)  Infrared sensors\nd)  Accelerometers\n",(0,o.jsx)(n.em,{children:"Answer: b)"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Siegwart, R., Nourbakhsh, I. R., & Scaramuzza, D. (2011). ",(0,o.jsx)(n.em,{children:"Introduction to Autonomous Mobile Robots"})," (2nd ed.). MIT Press."]}),"\n",(0,o.jsxs)(n.li,{children:["Forsyth, D. A., & Ponce, J. (2003). ",(0,o.jsx)(n.em,{children:"Computer Vision: A Modern Approach"}),". Prentice Hall."]}),"\n",(0,o.jsxs)(n.li,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,o.jsx)(n.em,{children:"Probabilistic Robotics"}),". MIT Press."]}),"\n",(0,o.jsxs)(n.li,{children:["Siciliano, B., Khatib, O., & SpringerLink. (2016). ",(0,o.jsx)(n.em,{children:"Springer Handbook of Robotics"})," (2nd ed.). Springer."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);