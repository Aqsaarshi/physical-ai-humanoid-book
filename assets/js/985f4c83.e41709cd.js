"use strict";(globalThis.webpackChunkphysical_ai_humanoid_book=globalThis.webpackChunkphysical_ai_humanoid_book||[]).push([[137],{5680:(e,n,t)=>{t.d(n,{xA:()=>u,yg:()=>f});var r=t(6540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){o(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},u=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef(function(e,n){var t=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=c(t),d=o,f=p["".concat(s,".").concat(d)]||p[d]||m[d]||i;return t?r.createElement(f,a(a({ref:n},u),{},{components:t})):r.createElement(f,a({ref:n},u))});function f(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=t.length,a=new Array(i);a[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:o,a[1]=l;for(var c=2;c<i;c++)a[c]=t[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},8898:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=t(8168),o=(t(6540),t(5680));const i={},a="Module 4: Vision-Language-Action (VLA)",l={unversionedId:"module4-vla",id:"module4-vla",title:"Module 4: Vision-Language-Action (VLA)",description:"Vision-Language-Action (VLA) models represent a frontier in AI, integrating capabilities from computer vision, natural language processing, and robotic control. These models enable robots to understand complex human commands, interpret visual information from their environment, and translate this understanding into physical actions. By combining visual perception with linguistic comprehension, VLA systems empower robots to interact more intuitively and effectively with the real world, performing tasks that require nuanced understanding and flexible execution.",source:"@site/docs/module4-vla.md",sourceDirName:".",slug:"/module4-vla",permalink:"/physical-ai-humanoid-book/docs/module4-vla",draft:!1,editUrl:"https://github.com/Aqsaarshi/physical-ai-humanoid-book/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 3: The AI-Robot Brain (NVIDIA Isaac)",permalink:"/physical-ai-humanoid-book/docs/module3-ai-robot-brain"},next:{title:"Module 1: The Robotic Nervous System (ROS 2)",permalink:"/physical-ai-humanoid-book/docs/module1-ros2"}},s={},c=[],u={toc:c},p="wrapper";function m({components:e,...n}){return(0,o.yg)(p,(0,r.A)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"module-4-vision-language-action-vla"},"Module 4: Vision-Language-Action (VLA)"),(0,o.yg)("p",null,"Vision-Language-Action (VLA) models represent a frontier in AI, integrating capabilities from computer vision, natural language processing, and robotic control. These models enable robots to understand complex human commands, interpret visual information from their environment, and translate this understanding into physical actions. By combining visual perception with linguistic comprehension, VLA systems empower robots to interact more intuitively and effectively with the real world, performing tasks that require nuanced understanding and flexible execution."))}m.isMDXComponent=!0}}]);