<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-chapter3" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 3: Sensing and Perception | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 3: Sensing and Perception | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="3.1 Introduction to Sensors"><meta data-rh="true" property="og:description" content="3.1 Introduction to Sensors"><link data-rh="true" rel="icon" href="/physical-ai-humanoid-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3"><link data-rh="true" rel="alternate" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3" hreflang="en"><link data-rh="true" rel="alternate" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 3: Sensing and Perception","item":"https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3"}]}</script><link rel="stylesheet" href="/physical-ai-humanoid-book/assets/css/styles.9f1d6320.css">
<script src="/physical-ai-humanoid-book/assets/js/runtime~main.caf15843.js" defer="defer"></script>
<script src="/physical-ai-humanoid-book/assets/js/main.116fb2e7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-humanoid-book/"><div class="navbar__logo"><img src="/physical-ai-humanoid-book/img/logo.jpg" alt="logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/physical-ai-humanoid-book/img/logo.jpg" alt="logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/physical-ai-humanoid-book/docs/introduction">Textbook</a><a class="navbar__item navbar__link" href="/physical-ai-humanoid-book/chatbot">chatbot</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="chatbot-container-wrapper"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/introduction"><span title="Feature Specification: Physical AI &amp; Humanoid Robotics Textbook" class="linkLabel_WmDU">Feature Specification: Physical AI &amp; Humanoid Robotics Textbook</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/implementation-plan"><span title="Implementation Plan: [FEATURE]" class="linkLabel_WmDU">Implementation Plan: [FEATURE]</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter1"><span title="Chapter 1: Introduction to Physical AI" class="linkLabel_WmDU">Chapter 1: Introduction to Physical AI</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter2"><span title="Chapter 2: Robotics Fundamentals" class="linkLabel_WmDU">Chapter 2: Robotics Fundamentals</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/physical-ai-humanoid-book/docs/chapter3"><span title="Chapter 3: Sensing and Perception" class="linkLabel_WmDU">Chapter 3: Sensing and Perception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter4"><span title="Chapter 4: Actuation and Control" class="linkLabel_WmDU">Chapter 4: Actuation and Control</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter5"><span title="Chapter 5: Robot Kinematics and Dynamics" class="linkLabel_WmDU">Chapter 5: Robot Kinematics and Dynamics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs"><span title="Welcome to the Physical AI &amp; Humanoid Robotics Textbook" class="linkLabel_WmDU">Welcome to the Physical AI &amp; Humanoid Robotics Textbook</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module1-ros2"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="linkLabel_WmDU">Module 1: The Robotic Nervous System (ROS 2)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module2-digital-twin"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="linkLabel_WmDU">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module3-ai-robot-brain"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac)" class="linkLabel_WmDU">Module 3: The AI-Robot Brain (NVIDIA Isaac)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module4-vla"><span title="Module 4: Vision-Language-Action (VLA)" class="linkLabel_WmDU">Module 4: Vision-Language-Action (VLA)</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/physical-ai-humanoid-book/docs/module1-ros2"><span title="New Modules" class="categoryLinkLabel_W154">New Modules</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/physical-ai-humanoid-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 3: Sensing and Perception</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 3: Sensing and Perception</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-introduction-to-sensors">3.1 Introduction to Sensors<a href="#31-introduction-to-sensors" class="hash-link" aria-label="Direct link to 3.1 Introduction to Sensors" title="Direct link to 3.1 Introduction to Sensors" translate="no">â€‹</a></h2>
<p>Sensors are critical components in any physical AI system, acting as the primary interface between the robot and its environment. They convert physical quantities (light, sound, pressure, distance, etc.) into electrical signals that the robot&#x27;s control system and AI algorithms can interpret.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-sensors">Types of Sensors<a href="#types-of-sensors" class="hash-link" aria-label="Direct link to Types of Sensors" title="Direct link to Types of Sensors" translate="no">â€‹</a></h3>
<p>Robots utilize a wide array of sensors, each designed for specific perception tasks:</p>
<ul>
<li class=""><strong>Vision Sensors</strong>: Cameras (monocular, stereo, RGB-D) provide visual information for object recognition, navigation, and scene understanding.</li>
<li class=""><strong>Proximity and Range Sensors</strong>: Ultrasonic sensors, infrared sensors, lidar, and radar are used to detect the presence of objects and measure distances.</li>
<li class=""><strong>Tactile and Force Sensors</strong>: Provide information about contact, pressure, and forces exerted during manipulation, enabling compliant interaction.</li>
<li class=""><strong>Proprioceptive Sensors</strong>: Internal sensors like encoders, accelerometers, and gyroscopes measure the robot&#x27;s own state, such as joint angles, velocity, and orientation.</li>
<li class=""><strong>Audio Sensors</strong>: Microphones for sound detection, speech recognition, and sound source localization.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-characteristics">Sensor Characteristics<a href="#sensor-characteristics" class="hash-link" aria-label="Direct link to Sensor Characteristics" title="Direct link to Sensor Characteristics" translate="no">â€‹</a></h3>
<p>Understanding sensor characteristics is vital for selecting the right sensor for a given task:</p>
<ul>
<li class=""><strong>Accuracy</strong>: How close a sensor&#x27;s measurement is to the true value.</li>
<li class=""><strong>Precision (Repeatability)</strong>: How close repeated measurements are to each other under the same conditions.</li>
<li class=""><strong>Resolution</strong>: The smallest change in the physical quantity that the sensor can detect.</li>
<li class=""><strong>Range</strong>: The minimum and maximum values that the sensor can measure.</li>
<li class=""><strong>Response Time</strong>: How quickly the sensor reacts to changes in the measured quantity.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-vision-systems">3.2 Vision Systems<a href="#32-vision-systems" class="hash-link" aria-label="Direct link to 3.2 Vision Systems" title="Direct link to 3.2 Vision Systems" translate="no">â€‹</a></h2>
<p>Vision systems are one of the most powerful perception modalities for robots, enabling them to &#x27;see&#x27; and interpret their surroundings.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cameras-monocular-stereo-depth">Cameras (Monocular, Stereo, Depth)<a href="#cameras-monocular-stereo-depth" class="hash-link" aria-label="Direct link to Cameras (Monocular, Stereo, Depth)" title="Direct link to Cameras (Monocular, Stereo, Depth)" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Monocular Cameras</strong>: Standard 2D cameras, providing rich texture and color information but lacking direct depth perception.</li>
<li class=""><strong>Stereo Cameras</strong>: Mimic human binocular vision using two cameras separated by a baseline to estimate depth through triangulation.</li>
<li class=""><strong>Depth Cameras (RGB-D)</strong>: Combine an RGB camera with a depth sensor (e.g., structured light, time-of-flight) to provide color images alongside per-pixel depth information.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="image-processing-fundamentals">Image Processing Fundamentals<a href="#image-processing-fundamentals" class="hash-link" aria-label="Direct link to Image Processing Fundamentals" title="Direct link to Image Processing Fundamentals" translate="no">â€‹</a></h3>
<p>Raw image data from cameras needs processing to extract meaningful information:</p>
<ul>
<li class=""><strong>Filtering</strong>: Noise reduction, edge detection, and smoothing.</li>
<li class=""><strong>Segmentation</strong>: Dividing an image into regions or objects.</li>
<li class=""><strong>Feature Extraction</strong>: Identifying key points, lines, or contours that represent salient aspects of the image.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="object-detection-and-recognition">Object Detection and Recognition<a href="#object-detection-and-recognition" class="hash-link" aria-label="Direct link to Object Detection and Recognition" title="Direct link to Object Detection and Recognition" translate="no">â€‹</a></h3>
<p>Advanced AI algorithms, particularly deep learning models (e.g., Convolutional Neural Networks), are used for:</p>
<ul>
<li class=""><strong>Object Detection</strong>: Identifying the presence and location of objects within an image (e.g., bounding boxes).</li>
<li class=""><strong>Object Recognition</strong>: Classifying detected objects into predefined categories.</li>
<li class=""><strong>Pose Estimation</strong>: Determining the 3D position and orientation of objects.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="33-other-perception-modalities">3.3 Other Perception Modalities<a href="#33-other-perception-modalities" class="hash-link" aria-label="Direct link to 3.3 Other Perception Modalities" title="Direct link to 3.3 Other Perception Modalities" translate="no">â€‹</a></h2>
<p>Beyond vision, other sensors provide crucial information, especially in conditions where vision is limited or insufficient.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lidar-and-radar">Lidar and Radar<a href="#lidar-and-radar" class="hash-link" aria-label="Direct link to Lidar and Radar" title="Direct link to Lidar and Radar" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Lidar (Light Detection and Ranging)</strong>: Uses pulsed laser light to measure distances to objects, creating detailed 3D maps of the environment. Excellent for autonomous navigation and mapping.</li>
<li class=""><strong>Radar (Radio Detection and Ranging)</strong>: Uses radio waves to detect objects and measure their velocity and distance. More robust in adverse weather conditions (fog, rain) than lidar or cameras.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="forcetorque-sensors">Force/Torque Sensors<a href="#forcetorque-sensors" class="hash-link" aria-label="Direct link to Force/Torque Sensors" title="Direct link to Force/Torque Sensors" translate="no">â€‹</a></h3>
<p>These sensors measure the forces and torques applied at a robot&#x27;s gripper or joints. They are essential for:</p>
<ul>
<li class=""><strong>Compliant Motion</strong>: Enabling robots to perform delicate tasks without damaging objects or exerting excessive force.</li>
<li class=""><strong>Human-Robot Interaction</strong>: Ensuring safety and natural interaction by detecting contact forces.</li>
<li class=""><strong>Manipulation</strong>: Allowing robots to handle objects with varying stiffness and weight.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="proprioceptive-sensors">Proprioceptive Sensors<a href="#proprioceptive-sensors" class="hash-link" aria-label="Direct link to Proprioceptive Sensors" title="Direct link to Proprioceptive Sensors" translate="no">â€‹</a></h3>
<p>Proprioceptive sensors provide feedback about the robot&#x27;s internal state, crucial for precise control and motion:</p>
<ul>
<li class=""><strong>Encoders</strong>: Measure the rotational or linear position of joints.</li>
<li class=""><strong>Accelerometers</strong>: Measure linear acceleration, useful for detecting movement and orientation changes.</li>
<li class=""><strong>Gyroscopes</strong>: Measure angular velocity, used for determining orientation and maintaining balance.</li>
<li class=""><strong>IMUs (Inertial Measurement Units)</strong>: Combine accelerometers and gyroscopes to provide comprehensive motion and orientation data.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="example">Example<a href="#example" class="hash-link" aria-label="Direct link to Example" title="Direct link to Example" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="basic-image-processing-for-robot-navigation">Basic Image Processing for Robot Navigation<a href="#basic-image-processing-for-robot-navigation" class="hash-link" aria-label="Direct link to Basic Image Processing for Robot Navigation" title="Direct link to Basic Image Processing for Robot Navigation" translate="no">â€‹</a></h3>
<p><strong>Concept</strong>: A common task for mobile robots is to navigate an environment using visual information. This example outlines a simplified image processing pipeline to detect a specific color (e.g., a green line or object) for navigation.</p>
<p><strong>Steps</strong>:</p>
<ol>
<li class="">
<p><strong>Image Acquisition</strong>: A robot&#x27;s monocular camera captures an image of the environment.</p>
<ul>
<li class=""><em>Input</em>: <code>color_image</code> (e.g., a NumPy array representing RGB pixel values).</li>
</ul>
</li>
<li class="">
<p><strong>Color Space Conversion</strong>: Convert the RGB image to a more suitable color space for color detection, such as HSV (Hue, Saturation, Value).</p>
<ul>
<li class=""><em>Reason</em>: HSV separates color information (Hue) from intensity (Value), making color thresholding more robust to lighting changes.</li>
<li class=""><em>Operation</em>: <code>hsv_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)</code> (using OpenCV).</li>
</ul>
</li>
<li class="">
<p><strong>Color Thresholding</strong>: Define a range for the target color in HSV space and create a binary mask. Pixels within the range become white (255), and others become black (0).</p>
<ul>
<li class=""><em>Example (Green Color)</em>:<!-- -->
<ul>
<li class=""><code>lower_green = np.array([30, 40, 40])</code></li>
<li class=""><code>upper_green = np.array([80, 255, 255])</code></li>
<li class=""><code>mask = cv2.inRange(hsv_image, lower_green, upper_green)</code></li>
</ul>
</li>
</ul>
</li>
<li class="">
<p><strong>Morphological Operations (Optional)</strong>: Clean up the mask by removing small noise (erosion) or filling small holes (dilation).</p>
<ul>
<li class=""><em>Operation</em>: <code>mask = cv2.erode(mask, kernel, iterations=1)</code> then <code>mask = cv2.dilate(mask, kernel, iterations=1)</code>.</li>
</ul>
</li>
<li class="">
<p><strong>Contour Detection</strong>: Find contours (outlines of objects) in the binary mask.</p>
<ul>
<li class=""><em>Operation</em>: <code>contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)</code>.</li>
</ul>
</li>
<li class="">
<p><strong>Object Localization</strong>: If a significant contour (e.g., the largest green object) is found, calculate its centroid or bounding box.</p>
<ul>
<li class=""><em>Decision</em>: Use the centroid&#x27;s horizontal position to determine if the robot needs to turn left, right, or go straight to center the object.</li>
</ul>
</li>
</ol>
<p><strong>Robot Action</strong>: Based on the localization of the green object, the robot&#x27;s control system would adjust motor speeds to steer towards or along the green path.</p>
<p>This simplified example demonstrates how raw sensor data (an image) is processed through various stages to extract actionable information for robot navigation.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="diagram">Diagram<a href="#diagram" class="hash-link" aria-label="Direct link to Diagram" title="Direct link to Diagram" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="figure-31-robot-sensor-suite-and-vision-processing-pipeline">Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline<a href="#figure-31-robot-sensor-suite-and-vision-processing-pipeline" class="hash-link" aria-label="Direct link to Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline" title="Direct link to Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline" translate="no">â€‹</a></h3>
<p><strong>Description</strong>: A block diagram showing two main parts:</p>
<p><strong>Part 1: Robot Sensor Suite</strong></p>
<ul>
<li class="">Illustrate a robot body with various sensors placed around it.</li>
<li class="">Labels should include: <strong>Camera</strong>, <strong>Lidar Sensor</strong>, <strong>Ultrasonic/IR Sensors</strong>, <strong>Force/Tactile Sensor (on gripper)</strong>, <strong>IMU (internal)</strong>.</li>
<li class="">Arrows should show data flowing from each sensor towards a central processing unit.</li>
</ul>
<p><strong>Part 2: Vision Processing Pipeline</strong></p>
<ul>
<li class="">A sequential flow starting with &#x27;<strong>Image Acquisition (Camera)</strong>&#x27;.</li>
<li class="">Flows to &#x27;<strong>Color Space Conversion (e.g., RGB to HSV)</strong>&#x27;.</li>
<li class="">Then to &#x27;<strong>Color Thresholding / Segmentation</strong>&#x27;.</li>
<li class="">Followed by &#x27;<strong>Morphological Operations (Erode/Dilate)</strong>&#x27;.</li>
<li class="">Leading to &#x27;<strong>Feature Extraction / Object Detection</strong>&#x27;.</li>
<li class="">Finally, results in &#x27;<strong>Actionable Information for Control (e.g., Turn Left/Right)</strong>&#x27;.</li>
</ul>
<p>Connect the output of the Sensor Suite to the input of the Vision Processing Pipeline, emphasizing how raw sensor data is transformed into meaningful information for robot decision-making.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="mini-task">Mini-Task<a href="#mini-task" class="hash-link" aria-label="Direct link to Mini-Task" title="Direct link to Mini-Task" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-selection-for-an-autonomous-mobile-robot">Sensor Selection for an Autonomous Mobile Robot<a href="#sensor-selection-for-an-autonomous-mobile-robot" class="hash-link" aria-label="Direct link to Sensor Selection for an Autonomous Mobile Robot" title="Direct link to Sensor Selection for an Autonomous Mobile Robot" translate="no">â€‹</a></h3>
<p><strong>Objective</strong>: Propose a sensor suite for an autonomous mobile robot (AMR) designed to navigate an indoor warehouse environment, avoiding obstacles and locating specific inventory items.</p>
<p><strong>Instructions</strong>:</p>
<ol>
<li class=""><strong>Identify key perception needs</strong>: What kind of information does the AMR need to gather from its environment (e.g., precise localization, obstacle detection, object identification)?</li>
<li class=""><strong>Select appropriate sensors</strong>: Based on the perception needs, choose a combination of sensors discussed in this chapter (e.g., Lidar, cameras, ultrasonic).</li>
<li class=""><strong>Justify your choices</strong>: Explain why each selected sensor is suitable for the AMR&#x27;s task, considering factors like environment, accuracy, and cost.</li>
</ol>
<p><strong>Example</strong>: For obstacle avoidance, a Lidar sensor provides precise 360-degree depth information, while ultrasonic sensors can detect nearby objects economically. A monocular camera with object detection AI can identify inventory items.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz">Quiz<a href="#quiz" class="hash-link" aria-label="Direct link to Quiz" title="Direct link to Quiz" translate="no">â€‹</a></h2>
<ol>
<li class="">
<p>Which sensor type is primarily used to provide visual information for object recognition and navigation?
a)  Ultrasonic sensors
b)  Force/Torque sensors
c)  Cameras
d)  Encoders
<em>Answer: c)</em></p>
</li>
<li class="">
<p>What does the term &#x27;resolution&#x27; refer to in the context of sensor characteristics?
a)  How close a sensor&#x27;s measurement is to the true value.
b)  How quickly the sensor reacts to changes.
c)  The smallest change in the physical quantity that the sensor can detect.
d)  The maximum range the sensor can measure.
<em>Answer: c)</em></p>
</li>
<li class="">
<p>Stereo cameras primarily estimate depth through which principle?
a)  Time-of-flight
b)  Structured light
c)  Triangulation
d)  Monocular vision
<em>Answer: c)</em></p>
</li>
<li class="">
<p>Which image processing operation is used to divide an image into regions or objects?
a)  Filtering
b)  Segmentation
c)  Feature Extraction
d)  Thresholding
<em>Answer: b)</em></p>
</li>
<li class="">
<p>Which sensor technology uses pulsed laser light to create detailed 3D maps of the environment and is excellent for autonomous navigation?
a)  Radar
b)  Lidar
c)  Infrared sensors
d)  Accelerometers
<em>Answer: b)</em></p>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">â€‹</a></h2>
<ul>
<li class="">Siegwart, R., Nourbakhsh, I. R., &amp; Scaramuzza, D. (2011). <em>Introduction to Autonomous Mobile Robots</em> (2nd ed.). MIT Press.</li>
<li class="">Forsyth, D. A., &amp; Ponce, J. (2003). <em>Computer Vision: A Modern Approach</em>. Prentice Hall.</li>
<li class="">Thrun, S., Burgard, W., &amp; Fox, D. (2005). <em>Probabilistic Robotics</em>. MIT Press.</li>
<li class="">Siciliano, B., Khatib, O., &amp; SpringerLink. (2016). <em>Springer Handbook of Robotics</em> (2nd ed.). Springer.</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book/tree/main/docs/chapter3.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/physical-ai-humanoid-book/docs/chapter2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 2: Robotics Fundamentals</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/physical-ai-humanoid-book/docs/chapter4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 4: Actuation and Control</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-introduction-to-sensors" class="table-of-contents__link toc-highlight">3.1 Introduction to Sensors</a><ul><li><a href="#types-of-sensors" class="table-of-contents__link toc-highlight">Types of Sensors</a></li><li><a href="#sensor-characteristics" class="table-of-contents__link toc-highlight">Sensor Characteristics</a></li></ul></li><li><a href="#32-vision-systems" class="table-of-contents__link toc-highlight">3.2 Vision Systems</a><ul><li><a href="#cameras-monocular-stereo-depth" class="table-of-contents__link toc-highlight">Cameras (Monocular, Stereo, Depth)</a></li><li><a href="#image-processing-fundamentals" class="table-of-contents__link toc-highlight">Image Processing Fundamentals</a></li><li><a href="#object-detection-and-recognition" class="table-of-contents__link toc-highlight">Object Detection and Recognition</a></li></ul></li><li><a href="#33-other-perception-modalities" class="table-of-contents__link toc-highlight">3.3 Other Perception Modalities</a><ul><li><a href="#lidar-and-radar" class="table-of-contents__link toc-highlight">Lidar and Radar</a></li><li><a href="#forcetorque-sensors" class="table-of-contents__link toc-highlight">Force/Torque Sensors</a></li><li><a href="#proprioceptive-sensors" class="table-of-contents__link toc-highlight">Proprioceptive Sensors</a></li></ul></li><li><a href="#example" class="table-of-contents__link toc-highlight">Example</a><ul><li><a href="#basic-image-processing-for-robot-navigation" class="table-of-contents__link toc-highlight">Basic Image Processing for Robot Navigation</a></li></ul></li><li><a href="#diagram" class="table-of-contents__link toc-highlight">Diagram</a><ul><li><a href="#figure-31-robot-sensor-suite-and-vision-processing-pipeline" class="table-of-contents__link toc-highlight">Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline</a></li></ul></li><li><a href="#mini-task" class="table-of-contents__link toc-highlight">Mini-Task</a><ul><li><a href="#sensor-selection-for-an-autonomous-mobile-robot" class="table-of-contents__link toc-highlight">Sensor Selection for an Autonomous Mobile Robot</a></li></ul></li><li><a href="#quiz" class="table-of-contents__link toc-highlight">Quiz</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div><button class="chatbot-toggle-button" style="position:fixed;bottom:20px;right:20px;z-index:1000;background-color:#007bff;color:white;border:none;border-radius:50%;width:60px;height:60px;font-size:24px;cursor:pointer;box-shadow:0 4px 8px rgba(0,0,0,0.2)">ðŸ’¬</button></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-book/docs/01-introduction">Textbook</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Physical AI & Humanoid Robotics Textbook.</div></div></div></footer></div>
</body>
</html>