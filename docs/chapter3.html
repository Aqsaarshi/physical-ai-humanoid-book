<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-chapter3">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<title data-rh="true">Chapter 3: Sensing and Perception | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 3: Sensing and Perception | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="3.1 Introduction to Sensors"><meta data-rh="true" property="og:description" content="3.1 Introduction to Sensors"><link data-rh="true" rel="icon" href="/physical-ai-humanoid-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3"><link data-rh="true" rel="alternate" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3" hreflang="en"><link data-rh="true" rel="alternate" href="https://Aqsaarshi.github.io/physical-ai-humanoid-book/docs/chapter3" hreflang="x-default"><link rel="stylesheet" href="/physical-ai-humanoid-book/assets/css/styles.1a0e38d6.css">
<link rel="preload" href="/physical-ai-humanoid-book/assets/js/runtime~main.eaa08216.js" as="script">
<link rel="preload" href="/physical-ai-humanoid-book/assets/js/main.cf81674f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-humanoid-book/"><div class="navbar__logo"><img src="/physical-ai-humanoid-book/img/logo.jpg" alt="logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/physical-ai-humanoid-book/img/logo.jpg" alt="logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/physical-ai-humanoid-book/docs/introduction">Textbook</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/introduction">Feature Specification: Physical AI &amp; Humanoid Robotics Textbook</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/implementation-plan">Implementation Plan: [FEATURE]</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter1">Chapter 1: Introduction to Physical AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter2">Chapter 2: Robotics Fundamentals</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/physical-ai-humanoid-book/docs/chapter3">Chapter 3: Sensing and Perception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter4">Chapter 4: Actuation and Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/chapter5">Chapter 5: Robot Kinematics and Dynamics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs">Welcome to the Physical AI &amp; Humanoid Robotics Textbook</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module1-ros2">Module 1: The Robotic Nervous System (ROS 2)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module2-digital-twin">Module 2: The Digital Twin (Gazebo &amp; Unity)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module3-ai-robot-brain">Module 3: The AI-Robot Brain (NVIDIA Isaac)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-book/docs/module4-vla">Module 4: Vision-Language-Action (VLA)</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/physical-ai-humanoid-book/docs/module1-ros2">New Modules</a></div></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_GujU"><div class="docItemContainer_Adtb"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/physical-ai-humanoid-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Chapter 3: Sensing and Perception</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_aoJ5"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Chapter 3: Sensing and Perception</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-introduction-to-sensors">3.1 Introduction to Sensors<a class="hash-link" href="#31-introduction-to-sensors" title="Direct link to heading">​</a></h2><p>Sensors are critical components in any physical AI system, acting as the primary interface between the robot and its environment. They convert physical quantities (light, sound, pressure, distance, etc.) into electrical signals that the robot&#x27;s control system and AI algorithms can interpret.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="types-of-sensors">Types of Sensors<a class="hash-link" href="#types-of-sensors" title="Direct link to heading">​</a></h3><p>Robots utilize a wide array of sensors, each designed for specific perception tasks:</p><ul><li><strong>Vision Sensors</strong>: Cameras (monocular, stereo, RGB-D) provide visual information for object recognition, navigation, and scene understanding.</li><li><strong>Proximity and Range Sensors</strong>: Ultrasonic sensors, infrared sensors, lidar, and radar are used to detect the presence of objects and measure distances.</li><li><strong>Tactile and Force Sensors</strong>: Provide information about contact, pressure, and forces exerted during manipulation, enabling compliant interaction.</li><li><strong>Proprioceptive Sensors</strong>: Internal sensors like encoders, accelerometers, and gyroscopes measure the robot&#x27;s own state, such as joint angles, velocity, and orientation.</li><li><strong>Audio Sensors</strong>: Microphones for sound detection, speech recognition, and sound source localization.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sensor-characteristics">Sensor Characteristics<a class="hash-link" href="#sensor-characteristics" title="Direct link to heading">​</a></h3><p>Understanding sensor characteristics is vital for selecting the right sensor for a given task:</p><ul><li><strong>Accuracy</strong>: How close a sensor&#x27;s measurement is to the true value.</li><li><strong>Precision (Repeatability)</strong>: How close repeated measurements are to each other under the same conditions.</li><li><strong>Resolution</strong>: The smallest change in the physical quantity that the sensor can detect.</li><li><strong>Range</strong>: The minimum and maximum values that the sensor can measure.</li><li><strong>Response Time</strong>: How quickly the sensor reacts to changes in the measured quantity.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-vision-systems">3.2 Vision Systems<a class="hash-link" href="#32-vision-systems" title="Direct link to heading">​</a></h2><p>Vision systems are one of the most powerful perception modalities for robots, enabling them to &#x27;see&#x27; and interpret their surroundings.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cameras-monocular-stereo-depth">Cameras (Monocular, Stereo, Depth)<a class="hash-link" href="#cameras-monocular-stereo-depth" title="Direct link to heading">​</a></h3><ul><li><strong>Monocular Cameras</strong>: Standard 2D cameras, providing rich texture and color information but lacking direct depth perception.</li><li><strong>Stereo Cameras</strong>: Mimic human binocular vision using two cameras separated by a baseline to estimate depth through triangulation.</li><li><strong>Depth Cameras (RGB-D)</strong>: Combine an RGB camera with a depth sensor (e.g., structured light, time-of-flight) to provide color images alongside per-pixel depth information.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-processing-fundamentals">Image Processing Fundamentals<a class="hash-link" href="#image-processing-fundamentals" title="Direct link to heading">​</a></h3><p>Raw image data from cameras needs processing to extract meaningful information:</p><ul><li><strong>Filtering</strong>: Noise reduction, edge detection, and smoothing.</li><li><strong>Segmentation</strong>: Dividing an image into regions or objects.</li><li><strong>Feature Extraction</strong>: Identifying key points, lines, or contours that represent salient aspects of the image.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-and-recognition">Object Detection and Recognition<a class="hash-link" href="#object-detection-and-recognition" title="Direct link to heading">​</a></h3><p>Advanced AI algorithms, particularly deep learning models (e.g., Convolutional Neural Networks), are used for:</p><ul><li><strong>Object Detection</strong>: Identifying the presence and location of objects within an image (e.g., bounding boxes).</li><li><strong>Object Recognition</strong>: Classifying detected objects into predefined categories.</li><li><strong>Pose Estimation</strong>: Determining the 3D position and orientation of objects.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-other-perception-modalities">3.3 Other Perception Modalities<a class="hash-link" href="#33-other-perception-modalities" title="Direct link to heading">​</a></h2><p>Beyond vision, other sensors provide crucial information, especially in conditions where vision is limited or insufficient.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lidar-and-radar">Lidar and Radar<a class="hash-link" href="#lidar-and-radar" title="Direct link to heading">​</a></h3><ul><li><strong>Lidar (Light Detection and Ranging)</strong>: Uses pulsed laser light to measure distances to objects, creating detailed 3D maps of the environment. Excellent for autonomous navigation and mapping.</li><li><strong>Radar (Radio Detection and Ranging)</strong>: Uses radio waves to detect objects and measure their velocity and distance. More robust in adverse weather conditions (fog, rain) than lidar or cameras.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forcetorque-sensors">Force/Torque Sensors<a class="hash-link" href="#forcetorque-sensors" title="Direct link to heading">​</a></h3><p>These sensors measure the forces and torques applied at a robot&#x27;s gripper or joints. They are essential for:</p><ul><li><strong>Compliant Motion</strong>: Enabling robots to perform delicate tasks without damaging objects or exerting excessive force.</li><li><strong>Human-Robot Interaction</strong>: Ensuring safety and natural interaction by detecting contact forces.</li><li><strong>Manipulation</strong>: Allowing robots to handle objects with varying stiffness and weight.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="proprioceptive-sensors">Proprioceptive Sensors<a class="hash-link" href="#proprioceptive-sensors" title="Direct link to heading">​</a></h3><p>Proprioceptive sensors provide feedback about the robot&#x27;s internal state, crucial for precise control and motion:</p><ul><li><strong>Encoders</strong>: Measure the rotational or linear position of joints.</li><li><strong>Accelerometers</strong>: Measure linear acceleration, useful for detecting movement and orientation changes.</li><li><strong>Gyroscopes</strong>: Measure angular velocity, used for determining orientation and maintaining balance.</li><li><strong>IMUs (Inertial Measurement Units)</strong>: Combine accelerometers and gyroscopes to provide comprehensive motion and orientation data.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="example">Example<a class="hash-link" href="#example" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="basic-image-processing-for-robot-navigation">Basic Image Processing for Robot Navigation<a class="hash-link" href="#basic-image-processing-for-robot-navigation" title="Direct link to heading">​</a></h3><p><strong>Concept</strong>: A common task for mobile robots is to navigate an environment using visual information. This example outlines a simplified image processing pipeline to detect a specific color (e.g., a green line or object) for navigation.</p><p><strong>Steps</strong>:</p><ol><li><p><strong>Image Acquisition</strong>: A robot&#x27;s monocular camera captures an image of the environment.</p><ul><li><em>Input</em>: <code>color_image</code> (e.g., a NumPy array representing RGB pixel values).</li></ul></li><li><p><strong>Color Space Conversion</strong>: Convert the RGB image to a more suitable color space for color detection, such as HSV (Hue, Saturation, Value).</p><ul><li><em>Reason</em>: HSV separates color information (Hue) from intensity (Value), making color thresholding more robust to lighting changes.</li><li><em>Operation</em>: <code>hsv_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)</code> (using OpenCV).</li></ul></li><li><p><strong>Color Thresholding</strong>: Define a range for the target color in HSV space and create a binary mask. Pixels within the range become white (255), and others become black (0).</p><ul><li><em>Example (Green Color)</em>:<ul><li><code>lower_green = np.array([30, 40, 40])</code></li><li><code>upper_green = np.array([80, 255, 255])</code></li><li><code>mask = cv2.inRange(hsv_image, lower_green, upper_green)</code></li></ul></li></ul></li><li><p><strong>Morphological Operations (Optional)</strong>: Clean up the mask by removing small noise (erosion) or filling small holes (dilation).</p><ul><li><em>Operation</em>: <code>mask = cv2.erode(mask, kernel, iterations=1)</code> then <code>mask = cv2.dilate(mask, kernel, iterations=1)</code>.</li></ul></li><li><p><strong>Contour Detection</strong>: Find contours (outlines of objects) in the binary mask.</p><ul><li><em>Operation</em>: <code>contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)</code>.</li></ul></li><li><p><strong>Object Localization</strong>: If a significant contour (e.g., the largest green object) is found, calculate its centroid or bounding box.</p><ul><li><em>Decision</em>: Use the centroid&#x27;s horizontal position to determine if the robot needs to turn left, right, or go straight to center the object.</li></ul></li></ol><p><strong>Robot Action</strong>: Based on the localization of the green object, the robot&#x27;s control system would adjust motor speeds to steer towards or along the green path.</p><p>This simplified example demonstrates how raw sensor data (an image) is processed through various stages to extract actionable information for robot navigation.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="diagram">Diagram<a class="hash-link" href="#diagram" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="figure-31-robot-sensor-suite-and-vision-processing-pipeline">Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline<a class="hash-link" href="#figure-31-robot-sensor-suite-and-vision-processing-pipeline" title="Direct link to heading">​</a></h3><p><strong>Description</strong>: A block diagram showing two main parts:</p><p><strong>Part 1: Robot Sensor Suite</strong></p><ul><li>Illustrate a robot body with various sensors placed around it.</li><li>Labels should include: <strong>Camera</strong>, <strong>Lidar Sensor</strong>, <strong>Ultrasonic/IR Sensors</strong>, <strong>Force/Tactile Sensor (on gripper)</strong>, <strong>IMU (internal)</strong>.</li><li>Arrows should show data flowing from each sensor towards a central processing unit.</li></ul><p><strong>Part 2: Vision Processing Pipeline</strong></p><ul><li>A sequential flow starting with &#x27;<strong>Image Acquisition (Camera)</strong>&#x27;.</li><li>Flows to &#x27;<strong>Color Space Conversion (e.g., RGB to HSV)</strong>&#x27;.</li><li>Then to &#x27;<strong>Color Thresholding / Segmentation</strong>&#x27;.</li><li>Followed by &#x27;<strong>Morphological Operations (Erode/Dilate)</strong>&#x27;.</li><li>Leading to &#x27;<strong>Feature Extraction / Object Detection</strong>&#x27;.</li><li>Finally, results in &#x27;<strong>Actionable Information for Control (e.g., Turn Left/Right)</strong>&#x27;.</li></ul><p>Connect the output of the Sensor Suite to the input of the Vision Processing Pipeline, emphasizing how raw sensor data is transformed into meaningful information for robot decision-making.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="mini-task">Mini-Task<a class="hash-link" href="#mini-task" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sensor-selection-for-an-autonomous-mobile-robot">Sensor Selection for an Autonomous Mobile Robot<a class="hash-link" href="#sensor-selection-for-an-autonomous-mobile-robot" title="Direct link to heading">​</a></h3><p><strong>Objective</strong>: Propose a sensor suite for an autonomous mobile robot (AMR) designed to navigate an indoor warehouse environment, avoiding obstacles and locating specific inventory items.</p><p><strong>Instructions</strong>:</p><ol><li><strong>Identify key perception needs</strong>: What kind of information does the AMR need to gather from its environment (e.g., precise localization, obstacle detection, object identification)?</li><li><strong>Select appropriate sensors</strong>: Based on the perception needs, choose a combination of sensors discussed in this chapter (e.g., Lidar, cameras, ultrasonic).</li><li><strong>Justify your choices</strong>: Explain why each selected sensor is suitable for the AMR&#x27;s task, considering factors like environment, accuracy, and cost.</li></ol><p><strong>Example</strong>: For obstacle avoidance, a Lidar sensor provides precise 360-degree depth information, while ultrasonic sensors can detect nearby objects economically. A monocular camera with object detection AI can identify inventory items.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quiz">Quiz<a class="hash-link" href="#quiz" title="Direct link to heading">​</a></h2><ol><li><p>Which sensor type is primarily used to provide visual information for object recognition and navigation?
a)  Ultrasonic sensors
b)  Force/Torque sensors
c)  Cameras
d)  Encoders
<em>Answer: c)</em></p></li><li><p>What does the term &#x27;resolution&#x27; refer to in the context of sensor characteristics?
a)  How close a sensor&#x27;s measurement is to the true value.
b)  How quickly the sensor reacts to changes.
c)  The smallest change in the physical quantity that the sensor can detect.
d)  The maximum range the sensor can measure.
<em>Answer: c)</em></p></li><li><p>Stereo cameras primarily estimate depth through which principle?
a)  Time-of-flight
b)  Structured light
c)  Triangulation
d)  Monocular vision
<em>Answer: c)</em></p></li><li><p>Which image processing operation is used to divide an image into regions or objects?
a)  Filtering
b)  Segmentation
c)  Feature Extraction
d)  Thresholding
<em>Answer: b)</em></p></li><li><p>Which sensor technology uses pulsed laser light to create detailed 3D maps of the environment and is excellent for autonomous navigation?
a)  Radar
b)  Lidar
c)  Infrared sensors
d)  Accelerometers
<em>Answer: b)</em></p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><ul><li>Siegwart, R., Nourbakhsh, I. R., &amp; Scaramuzza, D. (2011). <em>Introduction to Autonomous Mobile Robots</em> (2nd ed.). MIT Press.</li><li>Forsyth, D. A., &amp; Ponce, J. (2003). <em>Computer Vision: A Modern Approach</em>. Prentice Hall.</li><li>Thrun, S., Burgard, W., &amp; Fox, D. (2005). <em>Probabilistic Robotics</em>. MIT Press.</li><li>Siciliano, B., Khatib, O., &amp; SpringerLink. (2016). <em>Springer Handbook of Robotics</em> (2nd ed.). Springer.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book/tree/main/packages/create-docusaurus/templates/shared/docs/chapter3.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_eYIM" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vbeJ"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/physical-ai-humanoid-book/docs/chapter2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 2: Robotics Fundamentals</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/physical-ai-humanoid-book/docs/chapter4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 4: Actuation and Control</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-introduction-to-sensors" class="table-of-contents__link toc-highlight">3.1 Introduction to Sensors</a><ul><li><a href="#types-of-sensors" class="table-of-contents__link toc-highlight">Types of Sensors</a></li><li><a href="#sensor-characteristics" class="table-of-contents__link toc-highlight">Sensor Characteristics</a></li></ul></li><li><a href="#32-vision-systems" class="table-of-contents__link toc-highlight">3.2 Vision Systems</a><ul><li><a href="#cameras-monocular-stereo-depth" class="table-of-contents__link toc-highlight">Cameras (Monocular, Stereo, Depth)</a></li><li><a href="#image-processing-fundamentals" class="table-of-contents__link toc-highlight">Image Processing Fundamentals</a></li><li><a href="#object-detection-and-recognition" class="table-of-contents__link toc-highlight">Object Detection and Recognition</a></li></ul></li><li><a href="#33-other-perception-modalities" class="table-of-contents__link toc-highlight">3.3 Other Perception Modalities</a><ul><li><a href="#lidar-and-radar" class="table-of-contents__link toc-highlight">Lidar and Radar</a></li><li><a href="#forcetorque-sensors" class="table-of-contents__link toc-highlight">Force/Torque Sensors</a></li><li><a href="#proprioceptive-sensors" class="table-of-contents__link toc-highlight">Proprioceptive Sensors</a></li></ul></li><li><a href="#example" class="table-of-contents__link toc-highlight">Example</a><ul><li><a href="#basic-image-processing-for-robot-navigation" class="table-of-contents__link toc-highlight">Basic Image Processing for Robot Navigation</a></li></ul></li><li><a href="#diagram" class="table-of-contents__link toc-highlight">Diagram</a><ul><li><a href="#figure-31-robot-sensor-suite-and-vision-processing-pipeline" class="table-of-contents__link toc-highlight">Figure 3.1: Robot Sensor Suite and Vision Processing Pipeline</a></li></ul></li><li><a href="#mini-task" class="table-of-contents__link toc-highlight">Mini-Task</a><ul><li><a href="#sensor-selection-for-an-autonomous-mobile-robot" class="table-of-contents__link toc-highlight">Sensor Selection for an Autonomous Mobile Robot</a></li></ul></li><li><a href="#quiz" class="table-of-contents__link toc-highlight">Quiz</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-book/docs/01-introduction">Textbook</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Aqsaarshi/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Textbook, Built with Docusaurus.</div></div></div></footer></div>
<script src="/physical-ai-humanoid-book/assets/js/runtime~main.eaa08216.js"></script>
<script src="/physical-ai-humanoid-book/assets/js/main.cf81674f.js"></script>
</body>
</html>